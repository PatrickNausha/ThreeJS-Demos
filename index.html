<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0"><title>Patrick’s ThreeJS Demos</title><style>.demo:nth-child(2n+1){background:#f2f2f2}</style><link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin></head><body> <div class="container"> <p>For best results, open the demos in Google Chrome 99 or newer.</p> </div> <div class="container my-4 py-5"> <h1 class="mb-4 text-center">Patrick Nausha&rsquo;s ThreeJS Demos</h1> <p class="mb-4 text-center">The following are demo apps I created for the purpose of learning ThreeJS, WebGL, and 3D math.</p> <div class="d-flex"> <span class="mx-auto"> <a class="btn btn-md btn-primary me-3" href="mailto:patrick.nausha@gmail.com">Contact</a> <a class="btn btn-md btn-outline-primary" href="https://github.com/patricknausha/threejs-demos">View source</a> </span> </div> </div> <div class="demo py-5"> <div class="container"> <h2>Hologram Shader</h2> <div class="mb-3"><a class="btn btn-md btn-primary" href="hologram.html">View demo</a></div> <img class="img-fluid my-3" width="320" height="147" src="hologram-320.4ac6fa59.gif"> <h3> Goal </h3> <p> Implement a shader that can be used to render a <a href="https://starwars.fandom.com/wiki/Hologram">Star Wars</a>-inspired hologram. </p> <h3>Why?</h3> <p> Learn to write a GLSL shader. </p> <h3>Skills and techniques used</h3> <ul> <li>GLSL shaders</li> <li>Linear algebra</li> <li>Lambertian reflection</li> <li>Post-processing render passes</li> </ul> <h3>What I learned/what was new for me</h3> <p> This was my first time creating my own lighting. Since a hologram is intentionally not photorealistic, I decided I could use simple <a href="https://en.wikipedia.org/wiki/Lambertian_reflectance#Use_in_computer_graphics">Lambertian reflectance</a> alone. For this I used a hard-coded light direction and the normal vector from the vertex shader: </p> <p> <code class="code-block"> vec3 lightDirection = normalize(vec3(0.7, 0.5, 1.0));<br> float diffuse = max(dot(vNormal, lightDirection), 0.0); </code> </p> <p> I also learned to convert vectors from screen space to world space. In Star Wars, holograms have noise (presumably due to poor transmission). These can distort the image dramatically. We want a hologram's mesh geometry to occasionally skew and distort in a chaotic, but controlled, way. Try changing the "wiggle" settings in the demo to see which effect I am referring to. I offset a given mesh's vertices by a "noise shift" vector representing the amount of skewing caused by transmission noise. To make this shift look proper from every angle, I decided we should move the vertices in the distorted sections of the mesh to the viewer's left and right. This is in contrast to another option: Moving the geometry vertexes along a world space axis, e.g. east-west, to distort them. To figure out the new, distorted location for a vertex, I used the inverse view matrix to find a point in 3D world space to the viewer's left and right of the vertex's ordinary location. </p> <code class="code-block"> vec4 noiseShift = inverseViewMatrix * vec4(wiggleFactor * sin(x / 3.0) * sin(x / 13.0), 0.0, 0.0, 0.0);<br> vec3 shiftedPosition = noiseShift.xyz / 7.0 + position;<br> vec4 mvPosition = modelViewMatrix * vec4(shiftedPosition, 1.0);<br> gl_Position = projectionMatrix * mvPosition;</code> </div> </div> <div class="demo py-5"> <div class="container"> <h2>Reflecting an Object Across a Plane</h2> <div class="mb-3"><a class="btn btn-md btn-primary" href="fun-with-transformations-demo.html">View demo</a></div> <img class="img-fluid my-3" width="590" height="336" src="planar-reflection.59bb39ba.jpg"> <h3>Goal</h3> <p>Reflect an object across a 3D plane using matrix transformations.</p> <h3>Why?</h3> <p> Exercise my understanding of matrices and other 3D math concepts. </p> <h3>Skills used</h3> <ul> <li>Linear algebra</li> </ul> <h3>What I learned/what was new for me</h3> <p> Historically, a common solution for rendering reflections, e.g. an object reflected in a mirror, is to render the object a second time with its vertices transformed to the mirrored <a href="https://en.wikipedia.org/wiki/Virtual_image">virtual</a> location. I wanted to test my math skills with respect to 3D graphics/linear algebra by seeing if I could figure out how to do this transformation on my own. The "mirror" plane (red in the demo) translates and rotates within the 3D world. My solution ended up looking like the following (<code>reflection</code> is the reflected torus knot object). </p> <p> <code class="code-block"> const reflectionMatrix = new Matrix4()<br> &nbsp;&nbsp;&nbsp;&nbsp;.multiply(planeGroup.matrix) // Convert from plane-local space to world space.<br> &nbsp;&nbsp;&nbsp;&nbsp;.multiply(new Matrix4().makeScale(1, 1, -1)) // Mirror it.<br> &nbsp;&nbsp;&nbsp;&nbsp;.multiply(new Matrix4().getInverse(planeGroup.matrix)) // Convert world space to plane-local space.<br> &nbsp;&nbsp;&nbsp;&nbsp;.multiply(meshMatrixCopy); // Convert mesh-local space to world space.<br> reflection.matrix = reflectionMatrix; </code> </p> <p> In short, I discovered you can solve this problem by converting the mesh's world space coordinates to "mirror space" (a model space that is local to the mirror) and do the Z coordinate flipping (the mirroring operation) in "mirror space" coordinates. </p> </div> </div> <div class="demo py-5"> <div class="container"> <h2>Blurred Reflection</h2> <div class="mb-3"><a class="btn btn-md btn-primary" href="reflection-fun.html">View demo</a></div> <img class="img-fluid mb-3" width="360" height="318" src="reflection.e039e47e.gif"> <h3>Goal</h3> <p>Modify the <a href="https://github.com/mrdoob/three.js/blob/34dbd31e519f2b5458ebab2204ccbc0b9d0681e6/examples/jsm/objects/Reflector.js">ThreeJS example reflector</a> to support blurred reflections.</p> <h3>Why?</h3> <p> Learn to apply post-processing effects to texture render targets. Very few materials, except mirrors, reflect objects perfectly. Blurring a reflection slightly adds a level of realism.</p> <h3>Skills and techniques used</h3> <ul> <li>Texture render targets</li> <li>Post-processing effects</li> <li>Normal map</li> <li>Repeat texture wrapping</li> </ul> <h3>What I learned/what was new for me</h3> <p> I had applied post-processing effects in ThreeJS before, but those effects were applied to the final buffer rendered to the screen. The new challenge for me here was applying a post-processing effect to an intermediate texture render target. (Reflected objects are mirrored over the reflective plane and rendered separately to a texture.) I needed to apply my blur filter to that texture. Originally I tried implementing a <a href="https://en.wikipedia.org/wiki/Kernel_(image_processing)">convolution filter</a> in a shader. I later realized I could <a href="https://github.com/PatrickNausha/ThreeJS-Demos/commit/8503f9d181b7063c5eeaf63333c8ed346dc070df">use ThreeJS' existing bloom pass filter</a> as my blur filter. </p> </div> </div> <div class="demo py-5"> <div class="container"> <h2>&ldquo;Barrel&rdquo; Retro Game Asset as glTF File</h2> <div class="mb-3"><a class="btn btn-md btn-primary" href="gltf-barrel-demo.html">View demo</a></div> <img class="img-fluid my-3" width="340" height="219" src="barrel.5ed6e7d2.jpg"> <h3>Goal</h3> <p>Render an asset from a game I created in 2008.</p> <h3>Why?</h3> <p> Learn to render a glTF file in ThreeJS. Learn updated Blender UI. </p> <h4>Background</h4> <p> In high school I created a game using the <a href="https://irrlicht.sourceforge.io/">Irrlicht Engine</a>. I created some of the assets myself in Blender and Photoshop. </p> <h3>Skills and techniques used</h3> <ul> <li>Blender</li> <li>UV mapping</li> </ul> <h3>What I learned/what was new for me</h3> <p> The last time I used Blender was before Blender 2.8. Version 2.8 introduced tabbed workspaces at the top of the user interface. I found this super helpful for switching between tasks like modeling and UV editing. I ended up modifying the asset's UV map before re-exporting it as a <code>.gltf</code> file. The problem with the original asset is the texture was stretched along circumference of the barrel. I changed the UV coordinates to repeat the texture twice in a mirrored fashion around the barrel's circumference. While this adds a repeated pattern around the barrel, it causes more texels to be used per surface area of the barrel. This results in an apparent doubling of image resolution and a higher quality looking texture. </p> </div> </div> <div class="demo py-5"> <div class="container"> <h2>Presentation Title Screen</h2> <div class="mb-3"><a class="btn btn-md btn-primary" href="confetti-text-glow.html">View demo</a></div> <img class="img-fluid my-3" width="640" height="348" src="glow.d9722483.jpg"> <h3> Goal </h3> <p> Create a text slide that leverages real-time 3D graphics. </p> <h3> Why? </h3> <p> Solve a real-world problem using ThreeJS. </p> <h4>Background</h4> <p> <a href="https://faithlife.com">My current employer</a> creates <a href="https://faithlife.com/products/proclaim">Proclaim</a>—a church presentation software application—and <a href="https://media.faithlife.com/">sells media</a> to be used on-screen in churches, e.g. <a href="https://media.faithlife.com/media/6083-glowing-rain-countdown-3-min">this countdown</a>. Currently most of our graphics/media are dynamic 2D slides or static, pre-rendered 3D slides. What if instead Proclaim could leverage a broadly supported graphics API like WebGL or OpenGL ES to make its 3D content dynamic? Churches could modify the 3D graphics with custom text or other options. Proclaim could use <a href="https://en.wikipedia.org/wiki/Chromium_Embedded_Framework">CEF</a> to embed a WebGL page in the app and render WebGL scenes to a projector. Faithlife's <a href="https://faithlife.com/products/digital-signage">digital signage</a> system is already browser-based so the WebGL scenes should work for signs as well. </p> <h3>Skills and techniques used</h3> <ul> <li>Simple particle system</li> <li>Post-processing effects</li> </ul> <h3>What I learned/what was new for me</h3> <p> Unfortunately it turned out that the cost of creating these 3D dynamic scenes would not be economical for Faithlife. Faithlife can produce pre-rendered 2D and 3D static content cost-effectively using existing tools that do not require expensive software engineering effort. Building many of these 3D micro-scenes/apps may be too expensive to profit off of them. </p> </div> </div> <div class="demo py-5"> <div class="container"> <h2>Transparency, Depth Buffer, and Sorting Playground</h2> <div class="mb-3"><a class="btn btn-md btn-primary" href="fun-with-sorting-demo.html">View demo</a></div> <img class="img-fluid my-3" width="420" height="275" src="transparent-depth.53c139bf.jpg"> <h3> Goal </h3> <p>Create an application that lets you toggle meshes' transparencies, <a href="https://learnopengl.com/Advanced-OpenGL/Depth-testing">depth test</a> usage, and sorting.</p> <h3> Why? </h3> <p> Verify my understanding of how ThreeJS solves the problem of rendering multiple objects when some of the objects are translucent. </p> <h3>What I learned</h3> <p> Prior to creating this demo, I discovered that sometimes <a href="https://www.khronos.org/opengl/wiki/Transparency_Sorting#Translucency_and_the_depth_buffer">transparent objects don't play nice</a> with depth buffers and other transparent objects. To illustrate, ask yourself: How would you depth test opaque objects against translucent ones? What about when you're trying to draw an opaque object behind a translucent one? An ordinary depth test tries to <em>avoid</em> drawing farther objects on top of nearer ones. </p> <p> By building a demo where I can deconstruct and break things intentionally, I verified ThreeJS's solution to this problem: </p> <ul> <li>Draw opaque objects first.</li> <li>Draw translucent objects from farthest to nearest.</li> </ul> <p>Note that this strategy is not 100% effective because objects can occlude themselves.</p> </div> </div> <div class="demo py-5"> <div class="container"> <h2>SVG in 3D</h2> <div class="mb-3"><a class="btn btn-md btn-primary" href="svg.html">View demo</a></div> <img class="img-fluid my-3" width="640" height="329" src="svg.6e3f98f6.jpg"> <h3>Goal</h3> <p>Extrude an SVG file into a 3D object.</p> <h3>Why?</h3> <p> Gain familiarity with ThreeJS geometry APIs like <code>ShapeBufferGeometry</code> and <code>ExtrudeGeometry</code>. </p> <h3>What I learned/what was new for me</h3> <p> When building geometry from scratch, you don't get a lot for "free." <code>ExtrudeGeometry</code> for example doesn't generate normals that result in smooth shading. </p> </div> </div> <div class="demo py-5"> <div class="container"> <h2>Shadows</h2> <div class="mb-3"><a class="btn btn-md btn-primary" href="shadows.html">View demo</a></div> <img class="img-fluid my-3" width="640" height="350" src="shadows.54bcf65d.jpg"> <h3>Goal</h3> <p>Create a scene with several shadow-casting and shadow-receiving objects.</p> <h3>Why?</h3> <p> Learn ThreeJS basics. </p> <h3>Skills and techniques used</h3> <ul> <li>Point lights</li> <li>ThreeJS Percentage-Closer Filtering shadows</li> </ul> <h3>What I learned/what was new for me</h3> <p> This is one of the very first things I created with ThreeJS. I used this scene as an opportunity to learn ThreeJS' core APIs and learn what its performance may be like on old/mobile hardware when rendering shadows with very few shadow-casting lights. </p> </div> </div> </body></html>