<!DOCTYPE html>
<html>

<head>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0" />
	<title>Patrickâ€™s ThreeJS Demos</title>
	<style>
		body {
			font-family: Arial, Helvetica, sans-serif;
		}

		canvas {
			display: block;
		}

		.code-block {
			display: block;
		}
	</style>
</head>

<body>
	<p>For best results, open the demos in Google Chrome 99 or newer.</p>
	<h1>Patrick Nausha's ThreeJS demos</h1>
	<p>The following are demo apps I created for the purpose of learning ThreeJS, WebGL, and 3D math.
	</p>
	<h2><a href="./hologram.html">Hologram Shader</a></h2>
	<h3>
		Goal
	</h3>
	<p>
		Implement a shader that can be used to render a <a href="https://starwars.fandom.com/wiki/Hologram">Star
			Wars</a>-inspired hologram.
	</p>
	<h3>Why?</h3>
	<p>
		Learn to write a GLSL shader.
	</p>
	<h3>What I learned/what was new for me</h3>
	<p>
		This was my first time creating my own lighting. Since a hologram is intentionally not photorealistic, I decided
		I could use simple
		<a href="https://en.wikipedia.org/wiki/Lambertian_reflectance#Use_in_computer_graphics">Lambertian
			reflectance</a> alone. For this I used a hard-coded light direction and the normal vector from the vertex
		shader:
	</p>
	<code class="code-block">
		vec3 lightDirection = normalize(vec3(0.7, 0.5, 1.0));<br>
		float diffuse = max(dot(vNormal, lightDirection), 0.0);
	</code>
	<h4>Converting vectors from screen space to world space.</h4>
	<p>
		In Star Wars, holograms have noise (presumably due to poor transmission).
		These can distort the image dramatically.
		We want a hologram's mesh geometry to occasionally skew and distort in a chaotic, but controlled, way. Try
		changing the "wiggle" settings in the demo to see which effect I am referring to.
		I offset a given mesh's vertices by a "noise shift" vector representing the amount of skewing caused by
		transmission noise.
		To make this shift look proper from every angle, I decided we should move the vertices in the distorted sections
		of the mesh to the viewer's left and right.
		This is in contrast to another option: Moving the geometry vertexes along a wold space axis to distort them. To
		figure out the new,
		distorted location for a vertex, I used the inverse view matrix to find a point in 3D world
		space to the viewer's left and right of the vertex's ordinary location.
	</p>
	<code class="code-block">
		vec4 noiseShift = inverseViewMatrix * vec4(wiggleFactor * sin(x / 3.0) * sin(x / 13.0), 0.0, 0.0, 0.0);<br>
		vec3 shiftedPosition = noiseShift.xyz / 7.0 + position;<br>
		vec4 mvPosition = modelViewMatrix * vec4(shiftedPosition, 1.0);<br>
		gl_Position = projectionMatrix * mvPosition;
	</code>
	<h2><a href="./fun-with-transformations-demo.html">Reflecting an object across a plane</a></h2>
	<h3>Goal</h3>
	<p>Reflect an object across a 3D plane using matrix transformations.</p>
	<h3>Why?</h3>
	<p>
		Exercise my understanding of matrices and other 3D math concepts.
	</p>
	<h3>What I learned/what was new for me</h3>
	<p>
		Historically, a common solution for rendering reflections, e.g. an object reflected in a mirror, is to render
		the object a second time with its vertices transformed to the mirrored <a
			href="https://en.wikipedia.org/wiki/Virtual_image">virtual</a> location.
		I wanted to test my math skills with respect to 3D graphics/linear algebra by seeing if I could figure out how
		to do this transformation on my own. The "mirror" plane (red) translates and rotates within the 3D world.
		My solution ended up looking like the following. <code>reflection</code> is the reflected torus knot object.
	</p>
	<code class="code-block">
		const reflectionMatrix = new Matrix4()<br>
		&nbsp;&nbsp;&nbsp;&nbsp;.multiply(planeGroup.matrix) // Convert from plane-local space to world space.<br>
		&nbsp;&nbsp;&nbsp;&nbsp;.multiply(new Matrix4().makeScale(1, 1, -1)) // Mirror it.<br>
		&nbsp;&nbsp;&nbsp;&nbsp;.multiply(new Matrix4().getInverse(planeGroup.matrix)) // Convert world space to plane-local space.<br>
		&nbsp;&nbsp;&nbsp;&nbsp;.multiply(meshMatrixCopy); // Convert mesh-local space to world space.<br>
		reflection.matrix = reflectionMatrix;
	</code>
	<p>In short, I discovered you can solve this problem by converting the mesh's world space coordinates to "mirror
		space" (a model space that is local to the mirror) and do the Z coordinate flipping (the mirroring operation) in
		"mirror space" coordinates.
		Tip: If you're reading about 3D math and some multiplication operations look backwards to you, <a
			href="https://gamedev.stackexchange.com/questions/133529/difference-between-column-and-row-vector-matrix-multiplication-vector-transfor">the
			answer on this StackExchange question</a> may help you understand why.
	</p>

	<h2><a href="./reflection-fun.html">Blurred Reflection</a></h2>
	<h3>Goal</h3>
	<p>Modify the <a
			href="https://github.com/mrdoob/three.js/blob/34dbd31e519f2b5458ebab2204ccbc0b9d0681e6/examples/jsm/objects/Reflector.js#L1-L261">ThreeJS
			example reflector</a> to support blurred reflections.</p>
	<h3>Why?</h3>
	<p>
		Learn to apply postprocessing effects to texture render targets.
		Very few materials, except mirrors, reflect objects perfectly. Blurring a reflection slightly adds a level of
		realism.</p>
	<h3>What I learned/what was new for me</h3>
	<p>
		I had applied postprocessing effects in ThreeJS before, but those effects were applied to the final buffer
		rendered to the screen.
		The new challenge for me here was applying a post-processing effect to a texture render target.
		Reflected objects are mirrored over the reflective plane and rendered to a texture.
		I needed to apply my blur filter to that texture.
		Originally I tried implementing a <a href="https://en.wikipedia.org/wiki/Kernel_(image_processing)">convolution
			filter</a> in a shader. I later realized I could <a
			href="https://github.com/PatrickNausha/ThreeJS-Demos/commit/8503f9d181b7063c5eeaf63333c8ed346dc070df">use
			ThreeJS' existing bloom pass filter</a> instead of creating my own solution for the blur.
	</p>

	<h2><a href="./svg.html">SVG</a></h2>
	<h2><a href="./confetti-text-glow.html">Confetti, Text, and Glow</a></h2>
	<h3>
		Goal
	</h3>
	<p>
		Create a text slide that leverages real-time 3D graphics.
	</p>
	<h3>
		Why?
	</h3>
	<p>
		Solve a real-world problem using ThreeJS.
	</p>
	<h3>What I learned/what was new for me</h3>
	<p>
		<a href="https://faithlife.com">My current employer</a> creates <a
			href="https://faithlife.com/products/proclaim">Proclaim</a>, a church presentation software application
		and <a href="https://media.faithlife.com/">sells media</a> to be used on-screen in churches, e.g. <a
			href="https://media.faithlife.com/media/6083-glowing-rain-countdown-3-min">this countdown</a>.
		Currently most of our graphics/media are dynamic 2D slides or static, pre-rendered 3D slides.
		What if instead Proclaim could leverage a broadly supported graphics API like WebGL or OpenGL ES
		to make its 3D content dynamic? Churches could modify the 3D graphics with custom text or other options.
		Proclaim could use <a href="https://en.wikipedia.org/wiki/Chromium_Embedded_Framework">CEF</a> to
		embed a WebGL page in the app and render WebGL scenes to a projector.
		Faithlife's <a href="https://faithlife.com/products/digital-signage">digital signage</a> system is already
		browser-based
		so the WebGL scenes should work for signs as well.
	</p>
	<p>
		Unfortunately it turned out that the cost of creating these 3D dynamic scenes would not be economical for
		Faithlife.
		Faithlife can produce pre-rendered 2D and 3D static content cost-effectively using existing tools
		that do not require expensive software engineering effort. Building many of these 3D micro-scenes/apps
		may be too expensive to profit off of them.
	</p>
	<h2><a href="./fun-with-sorting-demo.html">Transparency, depth buffer, and sorting playground.</a></h2>
	<h3>
		Goal
	</h3>
	<p>Create an application that lets you toggle meshes' transparencies, <a
			href="https://learnopengl.com/Advanced-OpenGL/Depth-testing">depth test</a> usage, and sorting.</p>
	<h3>
		Why?
	</h3>
	<p>
		Verify my understanding of how ThreeJS solves the problem of rendering multiple objects when some of the objects
		are translucent.
	</p>
	<h3>What I learned</h3>
	<p>
		Prior to creating this demo, I discovered that sometimes <a
			href="https://www.khronos.org/opengl/wiki/Transparency_Sorting">transparent objects don't play nice</a> with
		depth buffers and other transparent objects. To illustrate, ask yourself: How would you depth test opaque
		objects against
		translucent ones?
		Your goal is to draw an opaque object behind a translucent one. In contrast, an ordinary depth test tries to
		<em>avoid</em> drawing farther objects on top of nearer ones.
		By building a demo where I can deconstruct and break things intentionally, I verified ThreeJS's solution to this
		problem:
	</p>
	<ul>
		<li>Draw opaque objects first.</li>
		<li>Draw translucent objects from farthest to nearest.</li>
	</ul>
	<p>Note that this strategy is not 100% effective because objects can occlude themselves.</p>

	<h2><a href="./shadows.html">Shadows</a></h2>
	<h3>Goal</h3>
	<p>Create a scene with several shadow-casting and shadow-receiving objects.</p>
	<h3>Why?</h3>
	<p>
		Learn ThreeJS basics.
	</p>
	<h3>What I learned/what was new for me</h3>
	<p>
		This is one of the very first things I created with ThreeJS.
		I used this scene as an opportunity to learn ThreeJS' core APIs
		and learn what its performance may be like on old/mobile hardware when rendering
		shadows with very few shadow-casting lights.
	</p>
</body>

</html>